{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871753e6",
   "metadata": {},
   "source": [
    "# Assignment 6 Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e90b6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec0beee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A  B\n",
       "0   5  5\n",
       "1  10  2\n",
       "2  15  3\n",
       "3  20  4\n",
       "4  25  5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data=[[5,5],[10,2],[15,3],[20,4],[25,5]], columns=['A','B'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b39905",
   "metadata": {},
   "source": [
    "### Min Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4002a9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [0.25      , 0.        ],\n",
       "       [0.5       , 0.33333333],\n",
       "       [0.75      , 0.66666667],\n",
       "       [1.        , 1.        ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar = MinMaxScaler()\n",
    "minmax_data = scalar.fit_transform(data)\n",
    "minmax_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f80512",
   "metadata": {},
   "source": [
    "### Scaling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "023046f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.preprocessing._data in sklearn.preprocessing:\n",
      "\n",
      "NAME\n",
      "    sklearn.preprocessing._data\n",
      "\n",
      "DESCRIPTION\n",
      "    # Authors: The scikit-learn developers\n",
      "    # SPDX-License-Identifier: BSD-3-Clause\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(sklearn.utils._repr_html.base.ReprHTMLMixin, sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin, sklearn.utils._metadata_requests._MetadataRequester)\n",
      "        Binarizer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        KernelCenterer(sklearn.base.ClassNamePrefixFeaturesOutMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        MaxAbsScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        MinMaxScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        Normalizer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        PowerTransformer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        QuantileTransformer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        RobustScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        StandardScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.ClassNamePrefixFeaturesOutMixin(builtins.object)\n",
      "        KernelCenterer(sklearn.base.ClassNamePrefixFeaturesOutMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.OneToOneFeatureMixin(builtins.object)\n",
      "        Binarizer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        MaxAbsScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        MinMaxScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        Normalizer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        PowerTransformer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        QuantileTransformer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        RobustScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        StandardScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.TransformerMixin(sklearn.utils._set_output._SetOutputMixin)\n",
      "        Binarizer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        KernelCenterer(sklearn.base.ClassNamePrefixFeaturesOutMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        MaxAbsScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        MinMaxScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        Normalizer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        PowerTransformer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        QuantileTransformer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        RobustScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        StandardScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.preprocessing._encoders._BaseEncoder(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._encoders.OneHotEncoder\n",
      "\n",
      "    class Binarizer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  Binarizer(*, threshold=0.0, copy=True)\n",
      "     |\n",
      "     |  Binarize data (set feature values to 0 or 1) according to a threshold.\n",
      "     |\n",
      "     |  Values greater than the threshold map to 1, while values less than\n",
      "     |  or equal to the threshold map to 0. With the default threshold of 0,\n",
      "     |  only positive values map to 1.\n",
      "     |\n",
      "     |  Binarization is a common operation on text count data where the\n",
      "     |  analyst can decide to only consider the presence or absence of a\n",
      "     |  feature rather than a quantified number of occurrences for instance.\n",
      "     |\n",
      "     |  It can also be used as a pre-processing step for estimators that\n",
      "     |  consider boolean random variables (e.g. modelled using the Bernoulli\n",
      "     |  distribution in a Bayesian setting).\n",
      "     |\n",
      "     |  Read more in the :ref:`User Guide <preprocessing_binarization>`.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  threshold : float, default=0.0\n",
      "     |      Feature values below or equal to this are replaced by 0, above it by 1.\n",
      "     |      Threshold may not be less than 0 for operations on sparse matrices.\n",
      "     |\n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace binarization and avoid a copy (if\n",
      "     |      the input is already a numpy array or a scipy.sparse CSR matrix).\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  binarize : Equivalent function without the estimator API.\n",
      "     |  KBinsDiscretizer : Bin continuous data into intervals.\n",
      "     |  OneHotEncoder : Encode categorical features as a one-hot numeric array.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  If the input is a sparse matrix, only the non-zero values are subject\n",
      "     |  to update by the :class:`Binarizer` class.\n",
      "     |\n",
      "     |  This estimator is :term:`stateless` and does not need to be fitted.\n",
      "     |  However, we recommend to call :meth:`fit_transform` instead of\n",
      "     |  :meth:`transform`, as parameter validation is only performed in\n",
      "     |  :meth:`fit`.\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import Binarizer\n",
      "     |  >>> X = [[ 1., -1.,  2.],\n",
      "     |  ...      [ 2.,  0.,  0.],\n",
      "     |  ...      [ 0.,  1., -1.]]\n",
      "     |  >>> transformer = Binarizer().fit(X)  # fit does nothing.\n",
      "     |  >>> transformer\n",
      "     |  Binarizer()\n",
      "     |  >>> transformer.transform(X)\n",
      "     |  array([[1., 0., 1.],\n",
      "     |         [1., 0., 0.],\n",
      "     |         [0., 1., 0.]])\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Binarizer\n",
      "     |      sklearn.base.OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, *, threshold=0.0, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __sklearn_tags__(self)\n",
      "     |\n",
      "     |  fit(self, X, y=None)\n",
      "     |      Only validates estimator's parameters.\n",
      "     |\n",
      "     |      This method allows to: (i) validate the estimator's parameters and\n",
      "     |      (ii) be consistent with the scikit-learn transformer API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted transformer.\n",
      "     |\n",
      "     |  set_transform_request(self: sklearn.preprocessing._data.Binarizer, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.Binarizer\n",
      "     |      Request metadata passed to the ``transform`` method.\n",
      "     |\n",
      "     |      Note that this method is only relevant if\n",
      "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      The options for each parameter are:\n",
      "     |\n",
      "     |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n",
      "     |\n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n",
      "     |\n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |\n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |\n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |\n",
      "     |      .. versionadded:: 1.3\n",
      "     |\n",
      "     |      .. note::\n",
      "     |          This method is only relevant if this estimator is used as a\n",
      "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``copy`` parameter in ``transform``.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |\n",
      "     |  transform(self, X, copy=None)\n",
      "     |      Binarize each element of X.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data to binarize, element by element.\n",
      "     |          scipy.sparse matrices should be in CSR format to avoid an\n",
      "     |          un-necessary copy.\n",
      "     |\n",
      "     |      copy : bool\n",
      "     |          Copy the input X or not.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |\n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then the following input feature names are generated:\n",
      "     |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |\n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |\n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |\n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |\n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |\n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |\n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |\n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |\n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  __sklearn_clone__(self)\n",
      "     |\n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |\n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |\n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |\n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |\n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "\n",
      "    class KernelCenterer(sklearn.base.ClassNamePrefixFeaturesOutMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  Center an arbitrary kernel matrix :math:`K`.\n",
      "     |\n",
      "     |  Let define a kernel :math:`K` such that:\n",
      "     |\n",
      "     |  .. math::\n",
      "     |      K(X, Y) = \\phi(X) . \\phi(Y)^{T}\n",
      "     |\n",
      "     |  :math:`\\phi(X)` is a function mapping of rows of :math:`X` to a\n",
      "     |  Hilbert space and :math:`K` is of shape `(n_samples, n_samples)`.\n",
      "     |\n",
      "     |  This class allows to compute :math:`\\tilde{K}(X, Y)` such that:\n",
      "     |\n",
      "     |  .. math::\n",
      "     |      \\tilde{K(X, Y)} = \\tilde{\\phi}(X) . \\tilde{\\phi}(Y)^{T}\n",
      "     |\n",
      "     |  :math:`\\tilde{\\phi}(X)` is the centered mapped data in the Hilbert\n",
      "     |  space.\n",
      "     |\n",
      "     |  `KernelCenterer` centers the features without explicitly computing the\n",
      "     |  mapping :math:`\\phi(\\cdot)`. Working with centered kernels is sometime\n",
      "     |  expected when dealing with algebra computation such as eigendecomposition\n",
      "     |  for :class:`~sklearn.decomposition.KernelPCA` for instance.\n",
      "     |\n",
      "     |  Read more in the :ref:`User Guide <kernel_centering>`.\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  K_fit_rows_ : ndarray of shape (n_samples,)\n",
      "     |      Average of each column of kernel matrix.\n",
      "     |\n",
      "     |  K_fit_all_ : float\n",
      "     |      Average of kernel matrix.\n",
      "     |\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.kernel_approximation.Nystroem : Approximate a kernel map\n",
      "     |      using a subset of the training data.\n",
      "     |\n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] `Schölkopf, Bernhard, Alexander Smola, and Klaus-Robert Müller.\n",
      "     |     \"Nonlinear component analysis as a kernel eigenvalue problem.\"\n",
      "     |     Neural computation 10.5 (1998): 1299-1319.\n",
      "     |     <https://www.mlpack.org/papers/kpca.pdf>`_\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import KernelCenterer\n",
      "     |  >>> from sklearn.metrics.pairwise import pairwise_kernels\n",
      "     |  >>> X = [[ 1., -2.,  2.],\n",
      "     |  ...      [ -2.,  1.,  3.],\n",
      "     |  ...      [ 4.,  1., -2.]]\n",
      "     |  >>> K = pairwise_kernels(X, metric='linear')\n",
      "     |  >>> K\n",
      "     |  array([[  9.,   2.,  -2.],\n",
      "     |         [  2.,  14., -13.],\n",
      "     |         [ -2., -13.,  21.]])\n",
      "     |  >>> transformer = KernelCenterer().fit(K)\n",
      "     |  >>> transformer\n",
      "     |  KernelCenterer()\n",
      "     |  >>> transformer.transform(K)\n",
      "     |  array([[  5.,   0.,  -5.],\n",
      "     |         [  0.,  14., -14.],\n",
      "     |         [ -5., -14.,  19.]])\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      KernelCenterer\n",
      "     |      sklearn.base.ClassNamePrefixFeaturesOutMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __sklearn_tags__(self)\n",
      "     |\n",
      "     |  fit(self, K, y=None)\n",
      "     |      Fit KernelCenterer.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      K : ndarray of shape (n_samples, n_samples)\n",
      "     |          Kernel matrix.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |\n",
      "     |  set_transform_request(self: sklearn.preprocessing._data.KernelCenterer, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.KernelCenterer\n",
      "     |      Request metadata passed to the ``transform`` method.\n",
      "     |\n",
      "     |      Note that this method is only relevant if\n",
      "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      The options for each parameter are:\n",
      "     |\n",
      "     |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n",
      "     |\n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n",
      "     |\n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |\n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |\n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |\n",
      "     |      .. versionadded:: 1.3\n",
      "     |\n",
      "     |      .. note::\n",
      "     |          This method is only relevant if this estimator is used as a\n",
      "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``copy`` parameter in ``transform``.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |\n",
      "     |  transform(self, K, copy=True)\n",
      "     |      Center kernel matrix.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      K : ndarray of shape (n_samples1, n_samples2)\n",
      "     |          Kernel matrix.\n",
      "     |\n",
      "     |      copy : bool, default=True\n",
      "     |          Set to False to perform inplace computation.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      K_new : ndarray of shape (n_samples1, n_samples2)\n",
      "     |          Returns the instance itself.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __annotations__ = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassNamePrefixFeaturesOutMixin:\n",
      "     |\n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |\n",
      "     |      The feature names out will prefixed by the lowercased class name. For\n",
      "     |      example, if the transformer outputs 3 features, then the feature names\n",
      "     |      out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Only used to validate feature names with the names seen in `fit`.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassNamePrefixFeaturesOutMixin:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |\n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |\n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |\n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |\n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |\n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |\n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |\n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |\n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  __sklearn_clone__(self)\n",
      "     |\n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |\n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |\n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |\n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |\n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "\n",
      "    class MaxAbsScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  MaxAbsScaler(*, copy=True)\n",
      "     |\n",
      "     |  Scale each feature by its maximum absolute value.\n",
      "     |\n",
      "     |  This estimator scales and translates each feature individually such\n",
      "     |  that the maximal absolute value of each feature in the\n",
      "     |  training set will be 1.0. It does not shift/center the data, and\n",
      "     |  thus does not destroy any sparsity.\n",
      "     |\n",
      "     |  This scaler can also be applied to sparse CSR or CSC matrices.\n",
      "     |\n",
      "     |  `MaxAbsScaler` doesn't reduce the effect of outliers; it only linearly\n",
      "     |  scales them down. For an example visualization, refer to :ref:`Compare\n",
      "     |  MaxAbsScaler with other scalers <plot_all_scaling_max_abs_scaler_section>`.\n",
      "     |\n",
      "     |  .. versionadded:: 0.17\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace scaling and avoid a copy (if the input\n",
      "     |      is already a numpy array).\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scale_ : ndarray of shape (n_features,)\n",
      "     |      Per feature relative scaling of the data.\n",
      "     |\n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *scale_* attribute.\n",
      "     |\n",
      "     |  max_abs_ : ndarray of shape (n_features,)\n",
      "     |      Per feature maximum absolute value.\n",
      "     |\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  n_samples_seen_ : int\n",
      "     |      The number of samples processed by the estimator. Will be reset on\n",
      "     |      new calls to fit, but increments across ``partial_fit`` calls.\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  maxabs_scale : Equivalent function without the estimator API.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "     |  transform.\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import MaxAbsScaler\n",
      "     |  >>> X = [[ 1., -1.,  2.],\n",
      "     |  ...      [ 2.,  0.,  0.],\n",
      "     |  ...      [ 0.,  1., -1.]]\n",
      "     |  >>> transformer = MaxAbsScaler().fit(X)\n",
      "     |  >>> transformer\n",
      "     |  MaxAbsScaler()\n",
      "     |  >>> transformer.transform(X)\n",
      "     |  array([[ 0.5, -1. ,  1. ],\n",
      "     |         [ 1. ,  0. ,  0. ],\n",
      "     |         [ 0. ,  1. , -0.5]])\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      MaxAbsScaler\n",
      "     |      sklearn.base.OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, *, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __sklearn_tags__(self)\n",
      "     |\n",
      "     |  fit(self, X, y=None)\n",
      "     |      Compute the maximum absolute value to be used for later scaling.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to compute the per-feature minimum and maximum\n",
      "     |          used for later scaling along the features axis.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |\n",
      "     |  inverse_transform(self, X)\n",
      "     |      Scale back the data to the original representation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data that should be transformed back.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_original : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  partial_fit(self, X, y=None)\n",
      "     |      Online computation of max absolute value of X for later scaling.\n",
      "     |\n",
      "     |      All of X is processed as a single batch. This is intended for cases\n",
      "     |      when :meth:`fit` is not feasible due to very large number of\n",
      "     |      `n_samples` or because X is read from a continuous stream.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to compute the mean and standard deviation\n",
      "     |          used for later scaling along the features axis.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |\n",
      "     |  transform(self, X)\n",
      "     |      Scale the data.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data that should be scaled.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |\n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then the following input feature names are generated:\n",
      "     |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |\n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |\n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |\n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |\n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |\n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |\n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |\n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |\n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  __sklearn_clone__(self)\n",
      "     |\n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |\n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |\n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |\n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |\n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "\n",
      "    class MinMaxScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  MinMaxScaler(feature_range=(0, 1), *, copy=True, clip=False)\n",
      "     |\n",
      "     |  Transform features by scaling each feature to a given range.\n",
      "     |\n",
      "     |  This estimator scales and translates each feature individually such\n",
      "     |  that it is in the given range on the training set, e.g. between\n",
      "     |  zero and one.\n",
      "     |\n",
      "     |  The transformation is given by::\n",
      "     |\n",
      "     |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      "     |      X_scaled = X_std * (max - min) + min\n",
      "     |\n",
      "     |  where min, max = feature_range.\n",
      "     |\n",
      "     |  This transformation is often used as an alternative to zero mean,\n",
      "     |  unit variance scaling.\n",
      "     |\n",
      "     |  `MinMaxScaler` doesn't reduce the effect of outliers, but it linearly\n",
      "     |  scales them down into a fixed range, where the largest occurring data point\n",
      "     |  corresponds to the maximum value and the smallest one corresponds to the\n",
      "     |  minimum value. For an example visualization, refer to :ref:`Compare\n",
      "     |  MinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.\n",
      "     |\n",
      "     |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  feature_range : tuple (min, max), default=(0, 1)\n",
      "     |      Desired range of transformed data.\n",
      "     |\n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace row normalization and avoid a\n",
      "     |      copy (if the input is already a numpy array).\n",
      "     |\n",
      "     |  clip : bool, default=False\n",
      "     |      Set to True to clip transformed values of held-out data to\n",
      "     |      provided `feature range`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  min_ : ndarray of shape (n_features,)\n",
      "     |      Per feature adjustment for minimum. Equivalent to\n",
      "     |      ``min - X.min(axis=0) * self.scale_``\n",
      "     |\n",
      "     |  scale_ : ndarray of shape (n_features,)\n",
      "     |      Per feature relative scaling of the data. Equivalent to\n",
      "     |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      "     |\n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *scale_* attribute.\n",
      "     |\n",
      "     |  data_min_ : ndarray of shape (n_features,)\n",
      "     |      Per feature minimum seen in the data\n",
      "     |\n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *data_min_*\n",
      "     |\n",
      "     |  data_max_ : ndarray of shape (n_features,)\n",
      "     |      Per feature maximum seen in the data\n",
      "     |\n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *data_max_*\n",
      "     |\n",
      "     |  data_range_ : ndarray of shape (n_features,)\n",
      "     |      Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      "     |\n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *data_range_*\n",
      "     |\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  n_samples_seen_ : int\n",
      "     |      The number of samples processed by the estimator.\n",
      "     |      It will be reset on new calls to fit, but increments across\n",
      "     |      ``partial_fit`` calls.\n",
      "     |\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  minmax_scale : Equivalent function without the estimator API.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "     |  transform.\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      "     |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      "     |  >>> scaler = MinMaxScaler()\n",
      "     |  >>> print(scaler.fit(data))\n",
      "     |  MinMaxScaler()\n",
      "     |  >>> print(scaler.data_max_)\n",
      "     |  [ 1. 18.]\n",
      "     |  >>> print(scaler.transform(data))\n",
      "     |  [[0.   0.  ]\n",
      "     |   [0.25 0.25]\n",
      "     |   [0.5  0.5 ]\n",
      "     |   [1.   1.  ]]\n",
      "     |  >>> print(scaler.transform([[2, 2]]))\n",
      "     |  [[1.5 0. ]]\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      MinMaxScaler\n",
      "     |      sklearn.base.OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, feature_range=(0, 1), *, copy=True, clip=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __sklearn_tags__(self)\n",
      "     |\n",
      "     |  fit(self, X, y=None)\n",
      "     |      Compute the minimum and maximum to be used for later scaling.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data used to compute the per-feature minimum and maximum\n",
      "     |          used for later scaling along the features axis.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |\n",
      "     |  inverse_transform(self, X)\n",
      "     |      Undo the scaling of X according to feature_range.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data that will be transformed. It cannot be sparse.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_original : ndarray of shape (n_samples, n_features)\n",
      "     |          Transformed data.\n",
      "     |\n",
      "     |  partial_fit(self, X, y=None)\n",
      "     |      Online computation of min and max on X for later scaling.\n",
      "     |\n",
      "     |      All of X is processed as a single batch. This is intended for cases\n",
      "     |      when :meth:`fit` is not feasible due to very large number of\n",
      "     |      `n_samples` or because X is read from a continuous stream.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data used to compute the mean and standard deviation\n",
      "     |          used for later scaling along the features axis.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |\n",
      "     |  transform(self, X)\n",
      "     |      Scale features of X according to feature_range.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data that will be transformed.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : ndarray of shape (n_samples, n_features)\n",
      "     |          Transformed data.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |\n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then the following input feature names are generated:\n",
      "     |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |\n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |\n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |\n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |\n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |\n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |\n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |\n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |\n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  __sklearn_clone__(self)\n",
      "     |\n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |\n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |\n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |\n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |\n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "\n",
      "    class Normalizer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  Normalizer(norm='l2', *, copy=True)\n",
      "     |\n",
      "     |  Normalize samples individually to unit norm.\n",
      "     |\n",
      "     |  Each sample (i.e. each row of the data matrix) with at least one\n",
      "     |  non zero component is rescaled independently of other samples so\n",
      "     |  that its norm (l1, l2 or inf) equals one.\n",
      "     |\n",
      "     |  This transformer is able to work both with dense numpy arrays and\n",
      "     |  scipy.sparse matrix (use CSR format if you want to avoid the burden of\n",
      "     |  a copy / conversion).\n",
      "     |\n",
      "     |  Scaling inputs to unit norms is a common operation for text\n",
      "     |  classification or clustering for instance. For instance the dot\n",
      "     |  product of two l2-normalized TF-IDF vectors is the cosine similarity\n",
      "     |  of the vectors and is the base similarity metric for the Vector\n",
      "     |  Space Model commonly used by the Information Retrieval community.\n",
      "     |\n",
      "     |  For an example visualization, refer to :ref:`Compare Normalizer with other\n",
      "     |  scalers <plot_all_scaling_normalizer_section>`.\n",
      "     |\n",
      "     |  Read more in the :ref:`User Guide <preprocessing_normalization>`.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  norm : {'l1', 'l2', 'max'}, default='l2'\n",
      "     |      The norm to use to normalize each non zero sample. If norm='max'\n",
      "     |      is used, values will be rescaled by the maximum of the absolute\n",
      "     |      values.\n",
      "     |\n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace row normalization and avoid a\n",
      "     |      copy (if the input is already a numpy array or a scipy.sparse\n",
      "     |      CSR matrix).\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  normalize : Equivalent function without the estimator API.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This estimator is :term:`stateless` and does not need to be fitted.\n",
      "     |  However, we recommend to call :meth:`fit_transform` instead of\n",
      "     |  :meth:`transform`, as parameter validation is only performed in\n",
      "     |  :meth:`fit`.\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import Normalizer\n",
      "     |  >>> X = [[4, 1, 2, 2],\n",
      "     |  ...      [1, 3, 9, 3],\n",
      "     |  ...      [5, 7, 5, 1]]\n",
      "     |  >>> transformer = Normalizer().fit(X)  # fit does nothing.\n",
      "     |  >>> transformer\n",
      "     |  Normalizer()\n",
      "     |  >>> transformer.transform(X)\n",
      "     |  array([[0.8, 0.2, 0.4, 0.4],\n",
      "     |         [0.1, 0.3, 0.9, 0.3],\n",
      "     |         [0.5, 0.7, 0.5, 0.1]])\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Normalizer\n",
      "     |      sklearn.base.OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, norm='l2', *, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __sklearn_tags__(self)\n",
      "     |\n",
      "     |  fit(self, X, y=None)\n",
      "     |      Only validates estimator's parameters.\n",
      "     |\n",
      "     |      This method allows to: (i) validate the estimator's parameters and\n",
      "     |      (ii) be consistent with the scikit-learn transformer API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data to estimate the normalization parameters.\n",
      "     |\n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted transformer.\n",
      "     |\n",
      "     |  set_transform_request(self: sklearn.preprocessing._data.Normalizer, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.Normalizer\n",
      "     |      Request metadata passed to the ``transform`` method.\n",
      "     |\n",
      "     |      Note that this method is only relevant if\n",
      "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      The options for each parameter are:\n",
      "     |\n",
      "     |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n",
      "     |\n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n",
      "     |\n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |\n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |\n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |\n",
      "     |      .. versionadded:: 1.3\n",
      "     |\n",
      "     |      .. note::\n",
      "     |          This method is only relevant if this estimator is used as a\n",
      "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``copy`` parameter in ``transform``.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |\n",
      "     |  transform(self, X, copy=None)\n",
      "     |      Scale each non zero row of X to unit norm.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data to normalize, row by row. scipy.sparse matrices should be\n",
      "     |          in CSR format to avoid an un-necessary copy.\n",
      "     |\n",
      "     |      copy : bool, default=None\n",
      "     |          Copy the input X or not.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |\n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then the following input feature names are generated:\n",
      "     |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |\n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |\n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |\n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |\n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |\n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |\n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |\n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |\n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  __sklearn_clone__(self)\n",
      "     |\n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |\n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |\n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |\n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |\n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "\n",
      "    class OneHotEncoder(_BaseEncoder)\n",
      "     |  OneHotEncoder(*, categories='auto', drop=None, sparse_output=True, dtype=<class 'numpy.float64'>, handle_unknown='error', min_frequency=None, max_categories=None, feature_name_combiner='concat')\n",
      "     |\n",
      "     |  Encode categorical features as a one-hot numeric array.\n",
      "     |\n",
      "     |  The input to this transformer should be an array-like of integers or\n",
      "     |  strings, denoting the values taken on by categorical (discrete) features.\n",
      "     |  The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n",
      "     |  encoding scheme. This creates a binary column for each category and\n",
      "     |  returns a sparse matrix or dense array (depending on the ``sparse_output``\n",
      "     |  parameter).\n",
      "     |\n",
      "     |  By default, the encoder derives the categories based on the unique values\n",
      "     |  in each feature. Alternatively, you can also specify the `categories`\n",
      "     |  manually.\n",
      "     |\n",
      "     |  This encoding is needed for feeding categorical data to many scikit-learn\n",
      "     |  estimators, notably linear models and SVMs with the standard kernels.\n",
      "     |\n",
      "     |  Note: a one-hot encoding of y labels should use a LabelBinarizer\n",
      "     |  instead.\n",
      "     |\n",
      "     |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      "     |  For a comparison of different encoders, refer to:\n",
      "     |  :ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  categories : 'auto' or a list of array-like, default='auto'\n",
      "     |      Categories (unique values) per feature:\n",
      "     |\n",
      "     |      - 'auto' : Determine categories automatically from the training data.\n",
      "     |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      "     |        column. The passed categories should not mix strings and numeric\n",
      "     |        values within a single feature, and should be sorted in case of\n",
      "     |        numeric values.\n",
      "     |\n",
      "     |      The used categories can be found in the ``categories_`` attribute.\n",
      "     |\n",
      "     |      .. versionadded:: 0.20\n",
      "     |\n",
      "     |  drop : {'first', 'if_binary'} or an array-like of shape (n_features,),             default=None\n",
      "     |      Specifies a methodology to use to drop one of the categories per\n",
      "     |      feature. This is useful in situations where perfectly collinear\n",
      "     |      features cause problems, such as when feeding the resulting data\n",
      "     |      into an unregularized linear regression model.\n",
      "     |\n",
      "     |      However, dropping one category breaks the symmetry of the original\n",
      "     |      representation and can therefore induce a bias in downstream models,\n",
      "     |      for instance for penalized linear classification or regression models.\n",
      "     |\n",
      "     |      - None : retain all features (the default).\n",
      "     |      - 'first' : drop the first category in each feature. If only one\n",
      "     |        category is present, the feature will be dropped entirely.\n",
      "     |      - 'if_binary' : drop the first category in each feature with two\n",
      "     |        categories. Features with 1 or more than 2 categories are\n",
      "     |        left intact.\n",
      "     |      - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n",
      "     |        should be dropped.\n",
      "     |\n",
      "     |      When `max_categories` or `min_frequency` is configured to group\n",
      "     |      infrequent categories, the dropping behavior is handled after the\n",
      "     |      grouping.\n",
      "     |\n",
      "     |      .. versionadded:: 0.21\n",
      "     |         The parameter `drop` was added in 0.21.\n",
      "     |\n",
      "     |      .. versionchanged:: 0.23\n",
      "     |         The option `drop='if_binary'` was added in 0.23.\n",
      "     |\n",
      "     |      .. versionchanged:: 1.1\n",
      "     |          Support for dropping infrequent categories.\n",
      "     |\n",
      "     |  sparse_output : bool, default=True\n",
      "     |      When ``True``, it returns a :class:`scipy.sparse.csr_matrix`,\n",
      "     |      i.e. a sparse matrix in \"Compressed Sparse Row\" (CSR) format.\n",
      "     |\n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `sparse` was renamed to `sparse_output`\n",
      "     |\n",
      "     |  dtype : number type, default=np.float64\n",
      "     |      Desired dtype of output.\n",
      "     |\n",
      "     |  handle_unknown : {'error', 'ignore', 'infrequent_if_exist', 'warn'},                      default='error'\n",
      "     |      Specifies the way unknown categories are handled during :meth:`transform`.\n",
      "     |\n",
      "     |      - 'error' : Raise an error if an unknown category is present during transform.\n",
      "     |      - 'ignore' : When an unknown category is encountered during\n",
      "     |        transform, the resulting one-hot encoded columns for this feature\n",
      "     |        will be all zeros. In the inverse transform, an unknown category\n",
      "     |        will be denoted as None.\n",
      "     |      - 'infrequent_if_exist' : When an unknown category is encountered\n",
      "     |        during transform, the resulting one-hot encoded columns for this\n",
      "     |        feature will map to the infrequent category if it exists. The\n",
      "     |        infrequent category will be mapped to the last position in the\n",
      "     |        encoding. During inverse transform, an unknown category will be\n",
      "     |        mapped to the category denoted `'infrequent'` if it exists. If the\n",
      "     |        `'infrequent'` category does not exist, then :meth:`transform` and\n",
      "     |        :meth:`inverse_transform` will handle an unknown category as with\n",
      "     |        `handle_unknown='ignore'`. Infrequent categories exist based on\n",
      "     |        `min_frequency` and `max_categories`. Read more in the\n",
      "     |        :ref:`User Guide <encoder_infrequent_categories>`.\n",
      "     |      - 'warn' : When an unknown category is encountered during transform\n",
      "     |        a warning is issued, and the encoding then proceeds as described for\n",
      "     |        `handle_unknown=\"infrequent_if_exist\"`.\n",
      "     |\n",
      "     |      .. versionchanged:: 1.1\n",
      "     |          `'infrequent_if_exist'` was added to automatically handle unknown\n",
      "     |          categories and infrequent categories.\n",
      "     |\n",
      "     |      .. versionadded:: 1.6\n",
      "     |         The option `\"warn\"` was added in 1.6.\n",
      "     |\n",
      "     |  min_frequency : int or float, default=None\n",
      "     |      Specifies the minimum frequency below which a category will be\n",
      "     |      considered infrequent.\n",
      "     |\n",
      "     |      - If `int`, categories with a smaller cardinality will be considered\n",
      "     |        infrequent.\n",
      "     |\n",
      "     |      - If `float`, categories with a smaller cardinality than\n",
      "     |        `min_frequency * n_samples`  will be considered infrequent.\n",
      "     |\n",
      "     |      .. versionadded:: 1.1\n",
      "     |          Read more in the :ref:`User Guide <encoder_infrequent_categories>`.\n",
      "     |\n",
      "     |  max_categories : int, default=None\n",
      "     |      Specifies an upper limit to the number of output features for each input\n",
      "     |      feature when considering infrequent categories. If there are infrequent\n",
      "     |      categories, `max_categories` includes the category representing the\n",
      "     |      infrequent categories along with the frequent categories. If `None`,\n",
      "     |      there is no limit to the number of output features.\n",
      "     |\n",
      "     |      .. versionadded:: 1.1\n",
      "     |          Read more in the :ref:`User Guide <encoder_infrequent_categories>`.\n",
      "     |\n",
      "     |  feature_name_combiner : \"concat\" or callable, default=\"concat\"\n",
      "     |      Callable with signature `def callable(input_feature, category)` that returns a\n",
      "     |      string. This is used to create feature names to be returned by\n",
      "     |      :meth:`get_feature_names_out`.\n",
      "     |\n",
      "     |      `\"concat\"` concatenates encoded feature name and category with\n",
      "     |      `feature + \"_\" + str(category)`.E.g. feature X with values 1, 6, 7 create\n",
      "     |      feature names `X_1, X_6, X_7`.\n",
      "     |\n",
      "     |      .. versionadded:: 1.3\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  categories_ : list of arrays\n",
      "     |      The categories of each feature determined during fitting\n",
      "     |      (in order of the features in X and corresponding with the output\n",
      "     |      of ``transform``). This includes the category specified in ``drop``\n",
      "     |      (if any).\n",
      "     |\n",
      "     |  drop_idx_ : array of shape (n_features,)\n",
      "     |      - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category\n",
      "     |        to be dropped for each feature.\n",
      "     |      - ``drop_idx_[i] = None`` if no category is to be dropped from the\n",
      "     |        feature with index ``i``, e.g. when `drop='if_binary'` and the\n",
      "     |        feature isn't binary.\n",
      "     |      - ``drop_idx_ = None`` if all the transformed features will be\n",
      "     |        retained.\n",
      "     |\n",
      "     |      If infrequent categories are enabled by setting `min_frequency` or\n",
      "     |      `max_categories` to a non-default value and `drop_idx[i]` corresponds\n",
      "     |      to a infrequent category, then the entire infrequent category is\n",
      "     |      dropped.\n",
      "     |\n",
      "     |      .. versionchanged:: 0.23\n",
      "     |         Added the possibility to contain `None` values.\n",
      "     |\n",
      "     |  infrequent_categories_ : list of ndarray\n",
      "     |      Defined only if infrequent categories are enabled by setting\n",
      "     |      `min_frequency` or `max_categories` to a non-default value.\n",
      "     |      `infrequent_categories_[i]` are the infrequent categories for feature\n",
      "     |      `i`. If the feature `i` has no infrequent categories\n",
      "     |      `infrequent_categories_[i]` is None.\n",
      "     |\n",
      "     |      .. versionadded:: 1.1\n",
      "     |\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  feature_name_combiner : callable or None\n",
      "     |      Callable with signature `def callable(input_feature, category)` that returns a\n",
      "     |      string. This is used to create feature names to be returned by\n",
      "     |      :meth:`get_feature_names_out`.\n",
      "     |\n",
      "     |      .. versionadded:: 1.3\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  OrdinalEncoder : Performs an ordinal (integer)\n",
      "     |    encoding of the categorical features.\n",
      "     |  TargetEncoder : Encodes categorical features using the target.\n",
      "     |  sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n",
      "     |    dictionary items (also handles string-valued features).\n",
      "     |  sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n",
      "     |    encoding of dictionary items or strings.\n",
      "     |  LabelBinarizer : Binarizes labels in a one-vs-all\n",
      "     |    fashion.\n",
      "     |  MultiLabelBinarizer : Transforms between iterable of\n",
      "     |    iterables and a multilabel format, e.g. a (samples x classes) binary\n",
      "     |    matrix indicating the presence of a class label.\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Given a dataset with two features, we let the encoder find the unique\n",
      "     |  values per feature and transform the data to a binary one-hot encoding.\n",
      "     |\n",
      "     |  >>> from sklearn.preprocessing import OneHotEncoder\n",
      "     |\n",
      "     |  One can discard categories not seen during `fit`:\n",
      "     |\n",
      "     |  >>> enc = OneHotEncoder(handle_unknown='ignore')\n",
      "     |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      "     |  >>> enc.fit(X)\n",
      "     |  OneHotEncoder(handle_unknown='ignore')\n",
      "     |  >>> enc.categories_\n",
      "     |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      "     |  >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
      "     |  array([[1., 0., 1., 0., 0.],\n",
      "     |         [0., 1., 0., 0., 0.]])\n",
      "     |  >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
      "     |  array([['Male', 1],\n",
      "     |         [None, 2]], dtype=object)\n",
      "     |  >>> enc.get_feature_names_out(['gender', 'group'])\n",
      "     |  array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'], ...)\n",
      "     |\n",
      "     |  One can always drop the first column for each feature:\n",
      "     |\n",
      "     |  >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n",
      "     |  >>> drop_enc.categories_\n",
      "     |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      "     |  >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      "     |  array([[0., 0., 0.],\n",
      "     |         [1., 1., 0.]])\n",
      "     |\n",
      "     |  Or drop a column for feature only having 2 categories:\n",
      "     |\n",
      "     |  >>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)\n",
      "     |  >>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      "     |  array([[0., 1., 0., 0.],\n",
      "     |         [1., 0., 1., 0.]])\n",
      "     |\n",
      "     |  One can change the way feature names are created.\n",
      "     |\n",
      "     |  >>> def custom_combiner(feature, category):\n",
      "     |  ...     return str(feature) + \"_\" + type(category).__name__ + \"_\" + str(category)\n",
      "     |  >>> custom_fnames_enc = OneHotEncoder(feature_name_combiner=custom_combiner).fit(X)\n",
      "     |  >>> custom_fnames_enc.get_feature_names_out()\n",
      "     |  array(['x0_str_Female', 'x0_str_Male', 'x1_int_1', 'x1_int_2', 'x1_int_3'],\n",
      "     |        dtype=object)\n",
      "     |\n",
      "     |  Infrequent categories are enabled by setting `max_categories` or `min_frequency`.\n",
      "     |\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3], dtype=object).T\n",
      "     |  >>> ohe = OneHotEncoder(max_categories=3, sparse_output=False).fit(X)\n",
      "     |  >>> ohe.infrequent_categories_\n",
      "     |  [array(['a', 'd'], dtype=object)]\n",
      "     |  >>> ohe.transform([[\"a\"], [\"b\"]])\n",
      "     |  array([[0., 0., 1.],\n",
      "     |         [1., 0., 0.]])\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      OneHotEncoder\n",
      "     |      _BaseEncoder\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, *, categories='auto', drop=None, sparse_output=True, dtype=<class 'numpy.float64'>, handle_unknown='error', min_frequency=None, max_categories=None, feature_name_combiner='concat')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit OneHotEncoder to X.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data to determine the categories of each feature.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted encoder.\n",
      "     |\n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |\n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then the following input feature names are generated:\n",
      "     |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names.\n",
      "     |\n",
      "     |  inverse_transform(self, X)\n",
      "     |      Convert the data back to the original representation.\n",
      "     |\n",
      "     |      When unknown categories are encountered (all zeros in the\n",
      "     |      one-hot encoding), ``None`` is used to represent this category. If the\n",
      "     |      feature with the unknown category has a dropped category, the dropped\n",
      "     |      category will be its inverse.\n",
      "     |\n",
      "     |      For a given input feature, if there is an infrequent category,\n",
      "     |      'infrequent_sklearn' will be used to represent the infrequent category.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      "     |          The transformed data.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_original : ndarray of shape (n_samples, n_features)\n",
      "     |          Inverse transformed array.\n",
      "     |\n",
      "     |  transform(self, X)\n",
      "     |      Transform X using one-hot encoding.\n",
      "     |\n",
      "     |      If `sparse_output=True` (default), it returns an instance of\n",
      "     |      :class:`scipy.sparse._csr.csr_matrix` (CSR format).\n",
      "     |\n",
      "     |      If there are infrequent categories for a feature, set by specifying\n",
      "     |      `max_categories` or `min_frequency`, the infrequent categories are\n",
      "     |      grouped into a single category.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data to encode.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_out : {ndarray, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      "     |          Transformed input. If `sparse_output=True`, a sparse matrix will be\n",
      "     |          returned.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseEncoder:\n",
      "     |\n",
      "     |  __sklearn_tags__(self)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseEncoder:\n",
      "     |\n",
      "     |  infrequent_categories_\n",
      "     |      Infrequent categories for each feature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |\n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |\n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |\n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |\n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |\n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |\n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |\n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |\n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |\n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |\n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  __sklearn_clone__(self)\n",
      "     |\n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |\n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |\n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |\n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |\n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "\n",
      "    class PowerTransformer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  PowerTransformer(method='yeo-johnson', *, standardize=True, copy=True)\n",
      "     |\n",
      "     |  Apply a power transform featurewise to make data more Gaussian-like.\n",
      "     |\n",
      "     |  Power transforms are a family of parametric, monotonic transformations\n",
      "     |  that are applied to make data more Gaussian-like. This is useful for\n",
      "     |  modeling issues related to heteroscedasticity (non-constant variance),\n",
      "     |  or other situations where normality is desired.\n",
      "     |\n",
      "     |  Currently, PowerTransformer supports the Box-Cox transform and the\n",
      "     |  Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n",
      "     |  minimizing skewness is estimated through maximum likelihood.\n",
      "     |\n",
      "     |  Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n",
      "     |  supports both positive or negative data.\n",
      "     |\n",
      "     |  By default, zero-mean, unit-variance normalization is applied to the\n",
      "     |  transformed data.\n",
      "     |\n",
      "     |  For an example visualization, refer to :ref:`Compare PowerTransformer with\n",
      "     |  other scalers <plot_all_scaling_power_transformer_section>`. To see the\n",
      "     |  effect of Box-Cox and Yeo-Johnson transformations on different\n",
      "     |  distributions, see:\n",
      "     |  :ref:`sphx_glr_auto_examples_preprocessing_plot_map_data_to_normal.py`.\n",
      "     |\n",
      "     |  Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      "     |\n",
      "     |  .. versionadded:: 0.20\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n",
      "     |      The power transform method. Available methods are:\n",
      "     |\n",
      "     |      - 'yeo-johnson' [1]_, works with positive and negative values\n",
      "     |      - 'box-cox' [2]_, only works with strictly positive values\n",
      "     |\n",
      "     |  standardize : bool, default=True\n",
      "     |      Set to True to apply zero-mean, unit-variance normalization to the\n",
      "     |      transformed output.\n",
      "     |\n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace computation during transformation.\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  lambdas_ : ndarray of float of shape (n_features,)\n",
      "     |      The parameters of the power transformation for the selected features.\n",
      "     |\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  power_transform : Equivalent function without the estimator API.\n",
      "     |\n",
      "     |  QuantileTransformer : Maps data to a standard normal distribution with\n",
      "     |      the parameter `output_distribution='normal'`.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  NaNs are treated as missing values: disregarded in ``fit``, and maintained\n",
      "     |  in ``transform``.\n",
      "     |\n",
      "     |  References\n",
      "     |  ----------\n",
      "     |\n",
      "     |  .. [1] :doi:`I.K. Yeo and R.A. Johnson, \"A new family of power\n",
      "     |         transformations to improve normality or symmetry.\" Biometrika,\n",
      "     |         87(4), pp.954-959, (2000). <10.1093/biomet/87.4.954>`\n",
      "     |\n",
      "     |  .. [2] :doi:`G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\",\n",
      "     |         Journal of the Royal Statistical Society B, 26, 211-252 (1964).\n",
      "     |         <10.1111/j.2517-6161.1964.tb00553.x>`\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.preprocessing import PowerTransformer\n",
      "     |  >>> pt = PowerTransformer()\n",
      "     |  >>> data = [[1, 2], [3, 2], [4, 5]]\n",
      "     |  >>> print(pt.fit(data))\n",
      "     |  PowerTransformer()\n",
      "     |  >>> print(pt.lambdas_)\n",
      "     |  [ 1.386 -3.100]\n",
      "     |  >>> print(pt.transform(data))\n",
      "     |  [[-1.316 -0.707]\n",
      "     |   [ 0.209 -0.707]\n",
      "     |   [ 1.106  1.414]]\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      PowerTransformer\n",
      "     |      sklearn.base.OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, method='yeo-johnson', *, standardize=True, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __sklearn_tags__(self)\n",
      "     |\n",
      "     |  fit(self, X, y=None)\n",
      "     |      Estimate the optimal parameter lambda for each feature.\n",
      "     |\n",
      "     |      The optimal lambda parameter for minimizing skewness is estimated on\n",
      "     |      each feature independently using maximum likelihood.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data used to estimate the optimal transformation parameters.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted transformer.\n",
      "     |\n",
      "     |  fit_transform(self, X, y=None)\n",
      "     |      Fit `PowerTransformer` to `X`, then transform `X`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data used to estimate the optimal transformation parameters\n",
      "     |          and to be transformed using a power transformation.\n",
      "     |\n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray of shape (n_samples, n_features)\n",
      "     |          Transformed data.\n",
      "     |\n",
      "     |  inverse_transform(self, X)\n",
      "     |      Apply the inverse power transformation using the fitted lambdas.\n",
      "     |\n",
      "     |      The inverse of the Box-Cox transformation is given by::\n",
      "     |\n",
      "     |          if lambda_ == 0:\n",
      "     |              X_original = exp(X_trans)\n",
      "     |          else:\n",
      "     |              X_original = (X * lambda_ + 1) ** (1 / lambda_)\n",
      "     |\n",
      "     |      The inverse of the Yeo-Johnson transformation is given by::\n",
      "     |\n",
      "     |          if X >= 0 and lambda_ == 0:\n",
      "     |              X_original = exp(X) - 1\n",
      "     |          elif X >= 0 and lambda_ != 0:\n",
      "     |              X_original = (X * lambda_ + 1) ** (1 / lambda_) - 1\n",
      "     |          elif X < 0 and lambda_ != 2:\n",
      "     |              X_original = 1 - (-(2 - lambda_) * X + 1) ** (1 / (2 - lambda_))\n",
      "     |          elif X < 0 and lambda_ == 2:\n",
      "     |              X_original = 1 - exp(-X)\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The transformed data.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_original : ndarray of shape (n_samples, n_features)\n",
      "     |          The original data.\n",
      "     |\n",
      "     |  transform(self, X)\n",
      "     |      Apply the power transform to each feature using the fitted lambdas.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data to be transformed using a power transformation.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_trans : ndarray of shape (n_samples, n_features)\n",
      "     |          The transformed data.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |\n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then the following input feature names are generated:\n",
      "     |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |\n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |\n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |\n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |\n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  __sklearn_clone__(self)\n",
      "     |\n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |\n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |\n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |\n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |\n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "\n",
      "    class QuantileTransformer(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  QuantileTransformer(*, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=10000, random_state=None, copy=True)\n",
      "     |\n",
      "     |  Transform features using quantiles information.\n",
      "     |\n",
      "     |  This method transforms the features to follow a uniform or a normal\n",
      "     |  distribution. Therefore, for a given feature, this transformation tends\n",
      "     |  to spread out the most frequent values. It also reduces the impact of\n",
      "     |  (marginal) outliers: this is therefore a robust preprocessing scheme.\n",
      "     |\n",
      "     |  The transformation is applied on each feature independently. First an\n",
      "     |  estimate of the cumulative distribution function of a feature is\n",
      "     |  used to map the original values to a uniform distribution. The obtained\n",
      "     |  values are then mapped to the desired output distribution using the\n",
      "     |  associated quantile function. Features values of new/unseen data that fall\n",
      "     |  below or above the fitted range will be mapped to the bounds of the output\n",
      "     |  distribution. Note that this transform is non-linear. It may distort linear\n",
      "     |  correlations between variables measured at the same scale but renders\n",
      "     |  variables measured at different scales more directly comparable.\n",
      "     |\n",
      "     |  For example visualizations, refer to :ref:`Compare QuantileTransformer with\n",
      "     |  other scalers <plot_all_scaling_quantile_transformer_section>`.\n",
      "     |\n",
      "     |  Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      "     |\n",
      "     |  .. versionadded:: 0.19\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_quantiles : int, default=1000 or n_samples\n",
      "     |      Number of quantiles to be computed. It corresponds to the number\n",
      "     |      of landmarks used to discretize the cumulative distribution function.\n",
      "     |      If n_quantiles is larger than the number of samples, n_quantiles is set\n",
      "     |      to the number of samples as a larger number of quantiles does not give\n",
      "     |      a better approximation of the cumulative distribution function\n",
      "     |      estimator.\n",
      "     |\n",
      "     |  output_distribution : {'uniform', 'normal'}, default='uniform'\n",
      "     |      Marginal distribution for the transformed data. The choices are\n",
      "     |      'uniform' (default) or 'normal'.\n",
      "     |\n",
      "     |  ignore_implicit_zeros : bool, default=False\n",
      "     |      Only applies to sparse matrices. If True, the sparse entries of the\n",
      "     |      matrix are discarded to compute the quantile statistics. If False,\n",
      "     |      these entries are treated as zeros.\n",
      "     |\n",
      "     |  subsample : int or None, default=10_000\n",
      "     |      Maximum number of samples used to estimate the quantiles for\n",
      "     |      computational efficiency. Note that the subsampling procedure may\n",
      "     |      differ for value-identical sparse and dense matrices.\n",
      "     |      Disable subsampling by setting `subsample=None`.\n",
      "     |\n",
      "     |      .. versionadded:: 1.5\n",
      "     |         The option `None` to disable subsampling was added.\n",
      "     |\n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Determines random number generation for subsampling and smoothing\n",
      "     |      noise.\n",
      "     |      Please see ``subsample`` for more details.\n",
      "     |      Pass an int for reproducible results across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |\n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace transformation and avoid a copy (if the\n",
      "     |      input is already a numpy array).\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_quantiles_ : int\n",
      "     |      The actual number of quantiles used to discretize the cumulative\n",
      "     |      distribution function.\n",
      "     |\n",
      "     |  quantiles_ : ndarray of shape (n_quantiles, n_features)\n",
      "     |      The values corresponding the quantiles of reference.\n",
      "     |\n",
      "     |  references_ : ndarray of shape (n_quantiles, )\n",
      "     |      Quantiles of references.\n",
      "     |\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  quantile_transform : Equivalent function without the estimator API.\n",
      "     |  PowerTransformer : Perform mapping to a normal distribution using a power\n",
      "     |      transform.\n",
      "     |  StandardScaler : Perform standardization that is faster, but less robust\n",
      "     |      to outliers.\n",
      "     |  RobustScaler : Perform robust standardization that removes the influence\n",
      "     |      of outliers but does not put outliers and inliers on the same scale.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "     |  transform.\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.preprocessing import QuantileTransformer\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n",
      "     |  >>> qt = QuantileTransformer(n_quantiles=10, random_state=0)\n",
      "     |  >>> qt.fit_transform(X)\n",
      "     |  array([...])\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      QuantileTransformer\n",
      "     |      sklearn.base.OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, *, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=10000, random_state=None, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __sklearn_tags__(self)\n",
      "     |\n",
      "     |  fit(self, X, y=None)\n",
      "     |      Compute the quantiles used for transforming.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the features axis. If a sparse\n",
      "     |          matrix is provided, it will be converted into a sparse\n",
      "     |          ``csc_matrix``. Additionally, the sparse matrix needs to be\n",
      "     |          nonnegative if `ignore_implicit_zeros` is False.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |         Fitted transformer.\n",
      "     |\n",
      "     |  inverse_transform(self, X)\n",
      "     |      Back-projection to the original space.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the features axis. If a sparse\n",
      "     |          matrix is provided, it will be converted into a sparse\n",
      "     |          ``csc_matrix``. Additionally, the sparse matrix needs to be\n",
      "     |          nonnegative if `ignore_implicit_zeros` is False.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_original : {ndarray, sparse matrix} of (n_samples, n_features)\n",
      "     |          The projected data.\n",
      "     |\n",
      "     |  transform(self, X)\n",
      "     |      Feature-wise transformation of the data.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the features axis. If a sparse\n",
      "     |          matrix is provided, it will be converted into a sparse\n",
      "     |          ``csc_matrix``. Additionally, the sparse matrix needs to be\n",
      "     |          nonnegative if `ignore_implicit_zeros` is False.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The projected data.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |\n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then the following input feature names are generated:\n",
      "     |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |\n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |\n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |\n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |\n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |\n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |\n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |\n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |\n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  __sklearn_clone__(self)\n",
      "     |\n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |\n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |\n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |\n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |\n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "\n",
      "    class RobustScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  RobustScaler(*, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)\n",
      "     |\n",
      "     |  Scale features using statistics that are robust to outliers.\n",
      "     |\n",
      "     |  This Scaler removes the median and scales the data according to\n",
      "     |  the quantile range (defaults to IQR: Interquartile Range).\n",
      "     |  The IQR is the range between the 1st quartile (25th quantile)\n",
      "     |  and the 3rd quartile (75th quantile).\n",
      "     |\n",
      "     |  Centering and scaling happen independently on each feature by\n",
      "     |  computing the relevant statistics on the samples in the training\n",
      "     |  set. Median and interquartile range are then stored to be used on\n",
      "     |  later data using the :meth:`transform` method.\n",
      "     |\n",
      "     |  Standardization of a dataset is a common preprocessing for many machine\n",
      "     |  learning estimators. Typically this is done by removing the mean and\n",
      "     |  scaling to unit variance. However, outliers can often influence the sample\n",
      "     |  mean / variance in a negative way. In such cases, using the median and the\n",
      "     |  interquartile range often give better results. For an example visualization\n",
      "     |  and comparison to other scalers, refer to :ref:`Compare RobustScaler with\n",
      "     |  other scalers <plot_all_scaling_robust_scaler_section>`.\n",
      "     |\n",
      "     |  .. versionadded:: 0.17\n",
      "     |\n",
      "     |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  with_centering : bool, default=True\n",
      "     |      If `True`, center the data before scaling.\n",
      "     |      This will cause :meth:`transform` to raise an exception when attempted\n",
      "     |      on sparse matrices, because centering them entails building a dense\n",
      "     |      matrix which in common use cases is likely to be too large to fit in\n",
      "     |      memory.\n",
      "     |\n",
      "     |  with_scaling : bool, default=True\n",
      "     |      If `True`, scale the data to interquartile range.\n",
      "     |\n",
      "     |  quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,         default=(25.0, 75.0)\n",
      "     |      Quantile range used to calculate `scale_`. By default this is equal to\n",
      "     |      the IQR, i.e., `q_min` is the first quantile and `q_max` is the third\n",
      "     |      quantile.\n",
      "     |\n",
      "     |      .. versionadded:: 0.18\n",
      "     |\n",
      "     |  copy : bool, default=True\n",
      "     |      If `False`, try to avoid a copy and do inplace scaling instead.\n",
      "     |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      "     |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      "     |      returned.\n",
      "     |\n",
      "     |  unit_variance : bool, default=False\n",
      "     |      If `True`, scale data so that normally distributed features have a\n",
      "     |      variance of 1. In general, if the difference between the x-values of\n",
      "     |      `q_max` and `q_min` for a standard normal distribution is greater\n",
      "     |      than 1, the dataset will be scaled down. If less than 1, the dataset\n",
      "     |      will be scaled up.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  center_ : array of floats\n",
      "     |      The median value for each feature in the training set.\n",
      "     |\n",
      "     |  scale_ : array of floats\n",
      "     |      The (scaled) interquartile range for each feature in the training set.\n",
      "     |\n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *scale_* attribute.\n",
      "     |\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  robust_scale : Equivalent function without the estimator API.\n",
      "     |  sklearn.decomposition.PCA : Further removes the linear correlation across\n",
      "     |      features with 'whiten=True'.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |\n",
      "     |  https://en.wikipedia.org/wiki/Median\n",
      "     |  https://en.wikipedia.org/wiki/Interquartile_range\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import RobustScaler\n",
      "     |  >>> X = [[ 1., -2.,  2.],\n",
      "     |  ...      [ -2.,  1.,  3.],\n",
      "     |  ...      [ 4.,  1., -2.]]\n",
      "     |  >>> transformer = RobustScaler().fit(X)\n",
      "     |  >>> transformer\n",
      "     |  RobustScaler()\n",
      "     |  >>> transformer.transform(X)\n",
      "     |  array([[ 0. , -2. ,  0. ],\n",
      "     |         [-1. ,  0. ,  0.4],\n",
      "     |         [ 1. ,  0. , -1.6]])\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      RobustScaler\n",
      "     |      sklearn.base.OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, *, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __sklearn_tags__(self)\n",
      "     |\n",
      "     |  fit(self, X, y=None)\n",
      "     |      Compute the median and quantiles to be used for scaling.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to compute the median and quantiles\n",
      "     |          used for later scaling along the features axis.\n",
      "     |\n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |\n",
      "     |  inverse_transform(self, X)\n",
      "     |      Scale back the data to the original representation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The rescaled data to be transformed back.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_original : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  transform(self, X)\n",
      "     |      Center and scale the data.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the specified axis.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |\n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then the following input feature names are generated:\n",
      "     |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |\n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |\n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |\n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |\n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |\n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |\n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |\n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |\n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  __sklearn_clone__(self)\n",
      "     |\n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |\n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |\n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |\n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |\n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "\n",
      "    class StandardScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
      "     |\n",
      "     |  Standardize features by removing the mean and scaling to unit variance.\n",
      "     |\n",
      "     |  The standard score of a sample `x` is calculated as:\n",
      "     |\n",
      "     |  .. code-block:: text\n",
      "     |\n",
      "     |      z = (x - u) / s\n",
      "     |\n",
      "     |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      "     |  and `s` is the standard deviation of the training samples or one if\n",
      "     |  `with_std=False`.\n",
      "     |\n",
      "     |  Centering and scaling happen independently on each feature by computing\n",
      "     |  the relevant statistics on the samples in the training set. Mean and\n",
      "     |  standard deviation are then stored to be used on later data using\n",
      "     |  :meth:`transform`.\n",
      "     |\n",
      "     |  Standardization of a dataset is a common requirement for many\n",
      "     |  machine learning estimators: they might behave badly if the\n",
      "     |  individual features do not more or less look like standard normally\n",
      "     |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      "     |\n",
      "     |  For instance many elements used in the objective function of\n",
      "     |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      "     |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      "     |  all features are centered around 0 and have variance in the same\n",
      "     |  order. If a feature has a variance that is orders of magnitude larger\n",
      "     |  than others, it might dominate the objective function and make the\n",
      "     |  estimator unable to learn from other features correctly as expected.\n",
      "     |\n",
      "     |  `StandardScaler` is sensitive to outliers, and the features may scale\n",
      "     |  differently from each other in the presence of outliers. For an example\n",
      "     |  visualization, refer to :ref:`Compare StandardScaler with other scalers\n",
      "     |  <plot_all_scaling_standard_scaler_section>`.\n",
      "     |\n",
      "     |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      "     |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      "     |\n",
      "     |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  copy : bool, default=True\n",
      "     |      If False, try to avoid a copy and do inplace scaling instead.\n",
      "     |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      "     |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      "     |      returned.\n",
      "     |\n",
      "     |  with_mean : bool, default=True\n",
      "     |      If True, center the data before scaling.\n",
      "     |      This does not work (and will raise an exception) when attempted on\n",
      "     |      sparse matrices, because centering them entails building a dense\n",
      "     |      matrix which in common use cases is likely to be too large to fit in\n",
      "     |      memory.\n",
      "     |\n",
      "     |  with_std : bool, default=True\n",
      "     |      If True, scale the data to unit variance (or equivalently,\n",
      "     |      unit standard deviation).\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scale_ : ndarray of shape (n_features,) or None\n",
      "     |      Per feature relative scaling of the data to achieve zero mean and unit\n",
      "     |      variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
      "     |      variance is zero, we can't achieve unit variance, and the data is left\n",
      "     |      as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
      "     |      when `with_std=False`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *scale_*\n",
      "     |\n",
      "     |  mean_ : ndarray of shape (n_features,) or None\n",
      "     |      The mean value for each feature in the training set.\n",
      "     |      Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n",
      "     |\n",
      "     |  var_ : ndarray of shape (n_features,) or None\n",
      "     |      The variance for each feature in the training set. Used to compute\n",
      "     |      `scale_`. Equal to ``None`` when ``with_mean=False`` and\n",
      "     |      ``with_std=False``.\n",
      "     |\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |\n",
      "     |      .. versionadded:: 0.24\n",
      "     |\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |\n",
      "     |      .. versionadded:: 1.0\n",
      "     |\n",
      "     |  n_samples_seen_ : int or ndarray of shape (n_features,)\n",
      "     |      The number of samples processed by the estimator for each feature.\n",
      "     |      If there are no missing samples, the ``n_samples_seen`` will be an\n",
      "     |      integer, otherwise it will be an array of dtype int. If\n",
      "     |      `sample_weights` are used it will be a float (if no missing data)\n",
      "     |      or an array of dtype float that sums the weights seen so far.\n",
      "     |      Will be reset on new calls to fit, but increments across\n",
      "     |      ``partial_fit`` calls.\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  scale : Equivalent function without the estimator API.\n",
      "     |\n",
      "     |  :class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
      "     |      correlation across features with 'whiten=True'.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "     |  transform.\n",
      "     |\n",
      "     |  We use a biased estimator for the standard deviation, equivalent to\n",
      "     |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      "     |  affect model performance.\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      "     |  >>> scaler = StandardScaler()\n",
      "     |  >>> print(scaler.fit(data))\n",
      "     |  StandardScaler()\n",
      "     |  >>> print(scaler.mean_)\n",
      "     |  [0.5 0.5]\n",
      "     |  >>> print(scaler.transform(data))\n",
      "     |  [[-1. -1.]\n",
      "     |   [-1. -1.]\n",
      "     |   [ 1.  1.]\n",
      "     |   [ 1.  1.]]\n",
      "     |  >>> print(scaler.transform([[2, 2]]))\n",
      "     |  [[3. 3.]]\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      StandardScaler\n",
      "     |      sklearn.base.OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __sklearn_tags__(self)\n",
      "     |\n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Compute the mean and std to be used for later scaling.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to compute the mean and standard deviation\n",
      "     |          used for later scaling along the features axis.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |\n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample.\n",
      "     |\n",
      "     |          .. versionadded:: 0.24\n",
      "     |             parameter *sample_weight* support to StandardScaler.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |\n",
      "     |  inverse_transform(self, X, copy=None)\n",
      "     |      Scale back the data to the original representation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the features axis.\n",
      "     |\n",
      "     |      copy : bool, default=None\n",
      "     |          Copy the input `X` or not.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_original : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  partial_fit(self, X, y=None, sample_weight=None)\n",
      "     |      Online computation of mean and std on X for later scaling.\n",
      "     |\n",
      "     |      All of X is processed as a single batch. This is intended for cases\n",
      "     |      when :meth:`fit` is not feasible due to very large number of\n",
      "     |      `n_samples` or because X is read from a continuous stream.\n",
      "     |\n",
      "     |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      "     |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      "     |      for computing the sample variance: Analysis and recommendations.\"\n",
      "     |      The American Statistician 37.3 (1983): 242-247:\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to compute the mean and standard deviation\n",
      "     |          used for later scaling along the features axis.\n",
      "     |\n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |\n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample.\n",
      "     |\n",
      "     |          .. versionadded:: 0.24\n",
      "     |             parameter *sample_weight* support to StandardScaler.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |\n",
      "     |  set_fit_request(self: sklearn.preprocessing._data.StandardScaler, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler\n",
      "     |      Request metadata passed to the ``fit`` method.\n",
      "     |\n",
      "     |      Note that this method is only relevant if\n",
      "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      The options for each parameter are:\n",
      "     |\n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |\n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |\n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |\n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |\n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |\n",
      "     |      .. versionadded:: 1.3\n",
      "     |\n",
      "     |      .. note::\n",
      "     |          This method is only relevant if this estimator is used as a\n",
      "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |\n",
      "     |  set_inverse_transform_request(self: sklearn.preprocessing._data.StandardScaler, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler\n",
      "     |      Request metadata passed to the ``inverse_transform`` method.\n",
      "     |\n",
      "     |      Note that this method is only relevant if\n",
      "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      The options for each parameter are:\n",
      "     |\n",
      "     |      - ``True``: metadata is requested, and passed to ``inverse_transform`` if provided. The request is ignored if metadata is not provided.\n",
      "     |\n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``inverse_transform``.\n",
      "     |\n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |\n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |\n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |\n",
      "     |      .. versionadded:: 1.3\n",
      "     |\n",
      "     |      .. note::\n",
      "     |          This method is only relevant if this estimator is used as a\n",
      "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``copy`` parameter in ``inverse_transform``.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |\n",
      "     |  set_partial_fit_request(self: sklearn.preprocessing._data.StandardScaler, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler\n",
      "     |      Request metadata passed to the ``partial_fit`` method.\n",
      "     |\n",
      "     |      Note that this method is only relevant if\n",
      "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      The options for each parameter are:\n",
      "     |\n",
      "     |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |\n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
      "     |\n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |\n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |\n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |\n",
      "     |      .. versionadded:: 1.3\n",
      "     |\n",
      "     |      .. note::\n",
      "     |          This method is only relevant if this estimator is used as a\n",
      "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |\n",
      "     |  set_transform_request(self: sklearn.preprocessing._data.StandardScaler, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler\n",
      "     |      Request metadata passed to the ``transform`` method.\n",
      "     |\n",
      "     |      Note that this method is only relevant if\n",
      "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      The options for each parameter are:\n",
      "     |\n",
      "     |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n",
      "     |\n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n",
      "     |\n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |\n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |\n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |\n",
      "     |      .. versionadded:: 1.3\n",
      "     |\n",
      "     |      .. note::\n",
      "     |          This method is only relevant if this estimator is used as a\n",
      "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``copy`` parameter in ``transform``.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |\n",
      "     |  transform(self, X, copy=None)\n",
      "     |      Perform standardization by centering and scaling.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the features axis.\n",
      "     |      copy : bool, default=None\n",
      "     |          Copy the input X or not.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |\n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then the following input feature names are generated:\n",
      "     |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |\n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |\n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |\n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |\n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |\n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |\n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |\n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |\n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |\n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  __sklearn_clone__(self)\n",
      "     |\n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |\n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |\n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |\n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |\n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "\n",
      "FUNCTIONS\n",
      "    add_dummy_feature(X, value=1.0)\n",
      "        Augment dataset with an additional dummy feature.\n",
      "\n",
      "        This is useful for fitting an intercept term with implementations which\n",
      "        cannot otherwise fit it directly.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Data.\n",
      "\n",
      "        value : float\n",
      "            Value to use for the dummy feature.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X : {ndarray, sparse matrix} of shape (n_samples, n_features + 1)\n",
      "            Same data with dummy feature added as first column.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.preprocessing import add_dummy_feature\n",
      "        >>> add_dummy_feature([[0, 1], [1, 0]])\n",
      "        array([[1., 0., 1.],\n",
      "               [1., 1., 0.]])\n",
      "\n",
      "    binarize(X, *, threshold=0.0, copy=True)\n",
      "        Boolean thresholding of array-like or scipy.sparse matrix.\n",
      "\n",
      "        Read more in the :ref:`User Guide <preprocessing_binarization>`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to binarize, element by element.\n",
      "            scipy.sparse matrices should be in CSR or CSC format to avoid an\n",
      "            un-necessary copy.\n",
      "\n",
      "        threshold : float, default=0.0\n",
      "            Feature values below or equal to this are replaced by 0, above it by 1.\n",
      "            Threshold may not be less than 0 for operations on sparse matrices.\n",
      "\n",
      "        copy : bool, default=True\n",
      "            If False, try to avoid a copy and binarize in place.\n",
      "            This is not guaranteed to always work in place; e.g. if the data is\n",
      "            a numpy array with an object dtype, a copy will be returned even with\n",
      "            copy=False.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Binarizer : Performs binarization using the Transformer API\n",
      "            (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.preprocessing import binarize\n",
      "        >>> X = [[0.4, 0.6, 0.5], [0.6, 0.1, 0.2]]\n",
      "        >>> binarize(X, threshold=0.5)\n",
      "        array([[0., 1., 0.],\n",
      "               [1., 0., 0.]])\n",
      "\n",
      "    maxabs_scale(X, *, axis=0, copy=True)\n",
      "        Scale each feature to the [-1, 1] range without breaking the sparsity.\n",
      "\n",
      "        This estimator scales each feature individually such\n",
      "        that the maximal absolute value of each feature in the\n",
      "        training set will be 1.0.\n",
      "\n",
      "        This scaler can also be applied to sparse CSR or CSC matrices.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data.\n",
      "\n",
      "        axis : {0, 1}, default=0\n",
      "            Axis used to scale along. If 0, independently scale each feature,\n",
      "            otherwise (if 1) scale each sample.\n",
      "\n",
      "        copy : bool, default=True\n",
      "            If False, try to avoid a copy and scale in place.\n",
      "            This is not guaranteed to always work in place; e.g. if the data is\n",
      "            a numpy array with an int dtype, a copy will be returned even with\n",
      "            copy=False.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "\n",
      "        .. warning:: Risk of data leak\n",
      "\n",
      "            Do not use :func:`~sklearn.preprocessing.maxabs_scale` unless you know\n",
      "            what you are doing. A common mistake is to apply it to the entire data\n",
      "            *before* splitting into training and test sets. This will bias the\n",
      "            model evaluation because information would have leaked from the test\n",
      "            set to the training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.MaxAbsScaler` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking: `pipe = make_pipeline(MaxAbsScaler(), LogisticRegression())`.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        MaxAbsScaler : Performs scaling to the [-1, 1] range using\n",
      "            the Transformer API (e.g. as part of a preprocessing\n",
      "            :class:`~sklearn.pipeline.Pipeline`).\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        NaNs are treated as missing values: disregarded to compute the statistics,\n",
      "        and maintained during the data transformation.\n",
      "\n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.preprocessing import maxabs_scale\n",
      "        >>> X = [[-2, 1, 2], [-1, 0, 1]]\n",
      "        >>> maxabs_scale(X, axis=0)  # scale each column independently\n",
      "        array([[-1. ,  1. ,  1. ],\n",
      "               [-0.5,  0. ,  0.5]])\n",
      "        >>> maxabs_scale(X, axis=1)  # scale each row independently\n",
      "        array([[-1. ,  0.5,  1. ],\n",
      "               [-1. ,  0. ,  1. ]])\n",
      "\n",
      "    minmax_scale(X, feature_range=(0, 1), *, axis=0, copy=True)\n",
      "        Transform features by scaling each feature to a given range.\n",
      "\n",
      "        This estimator scales and translates each feature individually such\n",
      "        that it is in the given range on the training set, i.e. between\n",
      "        zero and one.\n",
      "\n",
      "        The transformation is given by (when ``axis=0``)::\n",
      "\n",
      "            X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      "            X_scaled = X_std * (max - min) + min\n",
      "\n",
      "        where min, max = feature_range.\n",
      "\n",
      "        The transformation is calculated as (when ``axis=0``)::\n",
      "\n",
      "           X_scaled = scale * X + min - X.min(axis=0) * scale\n",
      "           where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n",
      "\n",
      "        This transformation is often used as an alternative to zero mean,\n",
      "        unit variance scaling.\n",
      "\n",
      "        Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *minmax_scale* function interface\n",
      "           to :class:`~sklearn.preprocessing.MinMaxScaler`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            The data.\n",
      "\n",
      "        feature_range : tuple (min, max), default=(0, 1)\n",
      "            Desired range of transformed data.\n",
      "\n",
      "        axis : {0, 1}, default=0\n",
      "            Axis used to scale along. If 0, independently scale each feature,\n",
      "            otherwise (if 1) scale each sample.\n",
      "\n",
      "        copy : bool, default=True\n",
      "            If False, try to avoid a copy and scale in place.\n",
      "            This is not guaranteed to always work in place; e.g. if the data is\n",
      "            a numpy array with an int dtype, a copy will be returned even with\n",
      "            copy=False.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_tr : ndarray of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "\n",
      "        .. warning:: Risk of data leak\n",
      "\n",
      "            Do not use :func:`~sklearn.preprocessing.minmax_scale` unless you know\n",
      "            what you are doing. A common mistake is to apply it to the entire data\n",
      "            *before* splitting into training and test sets. This will bias the\n",
      "            model evaluation because information would have leaked from the test\n",
      "            set to the training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.MinMaxScaler` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking: `pipe = make_pipeline(MinMaxScaler(), LogisticRegression())`.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        MinMaxScaler : Performs scaling to a given range using the Transformer\n",
      "            API (e.g. as part of a preprocessing\n",
      "            :class:`~sklearn.pipeline.Pipeline`).\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.preprocessing import minmax_scale\n",
      "        >>> X = [[-2, 1, 2], [-1, 0, 1]]\n",
      "        >>> minmax_scale(X, axis=0)  # scale each column independently\n",
      "        array([[0., 1., 1.],\n",
      "               [1., 0., 0.]])\n",
      "        >>> minmax_scale(X, axis=1)  # scale each row independently\n",
      "        array([[0.  , 0.75, 1.  ],\n",
      "               [0.  , 0.5 , 1.  ]])\n",
      "\n",
      "    normalize(X, norm='l2', *, axis=1, copy=True, return_norm=False)\n",
      "        Scale input vectors individually to unit norm (vector length).\n",
      "\n",
      "        Read more in the :ref:`User Guide <preprocessing_normalization>`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to normalize, element by element.\n",
      "            scipy.sparse matrices should be in CSR format to avoid an\n",
      "            un-necessary copy.\n",
      "\n",
      "        norm : {'l1', 'l2', 'max'}, default='l2'\n",
      "            The norm to use to normalize each non zero sample (or each non-zero\n",
      "            feature if axis is 0).\n",
      "\n",
      "        axis : {0, 1}, default=1\n",
      "            Define axis used to normalize the data along. If 1, independently\n",
      "            normalize each sample, otherwise (if 0) normalize each feature.\n",
      "\n",
      "        copy : bool, default=True\n",
      "            If False, try to avoid a copy and normalize in place.\n",
      "            This is not guaranteed to always work in place; e.g. if the data is\n",
      "            a numpy array with an int dtype, a copy will be returned even with\n",
      "            copy=False.\n",
      "\n",
      "        return_norm : bool, default=False\n",
      "            Whether to return the computed norms.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            Normalized input X.\n",
      "\n",
      "        norms : ndarray of shape (n_samples, ) if axis=1 else (n_features, )\n",
      "            An array of norms along given axis for X.\n",
      "            When X is sparse, a NotImplementedError will be raised\n",
      "            for norm 'l1' or 'l2'.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Normalizer : Performs normalization using the Transformer API\n",
      "            (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.preprocessing import normalize\n",
      "        >>> X = [[-2, 1, 2], [-1, 0, 1]]\n",
      "        >>> normalize(X, norm=\"l1\")  # L1 normalization each row independently\n",
      "        array([[-0.4,  0.2,  0.4],\n",
      "               [-0.5,  0. ,  0.5]])\n",
      "        >>> normalize(X, norm=\"l2\")  # L2 normalization each row independently\n",
      "        array([[-0.67, 0.33, 0.67],\n",
      "               [-0.71, 0.  , 0.71]])\n",
      "\n",
      "    power_transform(X, method='yeo-johnson', *, standardize=True, copy=True)\n",
      "        Parametric, monotonic transformation to make data more Gaussian-like.\n",
      "\n",
      "        Power transforms are a family of parametric, monotonic transformations\n",
      "        that are applied to make data more Gaussian-like. This is useful for\n",
      "        modeling issues related to heteroscedasticity (non-constant variance),\n",
      "        or other situations where normality is desired.\n",
      "\n",
      "        Currently, power_transform supports the Box-Cox transform and the\n",
      "        Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n",
      "        minimizing skewness is estimated through maximum likelihood.\n",
      "\n",
      "        Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n",
      "        supports both positive or negative data.\n",
      "\n",
      "        By default, zero-mean, unit-variance normalization is applied to the\n",
      "        transformed data.\n",
      "\n",
      "        Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            The data to be transformed using a power transformation.\n",
      "\n",
      "        method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n",
      "            The power transform method. Available methods are:\n",
      "\n",
      "            - 'yeo-johnson' [1]_, works with positive and negative values\n",
      "            - 'box-cox' [2]_, only works with strictly positive values\n",
      "\n",
      "            .. versionchanged:: 0.23\n",
      "                The default value of the `method` parameter changed from\n",
      "                'box-cox' to 'yeo-johnson' in 0.23.\n",
      "\n",
      "        standardize : bool, default=True\n",
      "            Set to True to apply zero-mean, unit-variance normalization to the\n",
      "            transformed output.\n",
      "\n",
      "        copy : bool, default=True\n",
      "            If False, try to avoid a copy and transform in place.\n",
      "            This is not guaranteed to always work in place; e.g. if the data is\n",
      "            a numpy array with an int dtype, a copy will be returned even with\n",
      "            copy=False.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_trans : ndarray of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        PowerTransformer : Equivalent transformation with the\n",
      "            Transformer API (e.g. as part of a preprocessing\n",
      "            :class:`~sklearn.pipeline.Pipeline`).\n",
      "\n",
      "        quantile_transform : Maps data to a standard normal distribution with\n",
      "            the parameter `output_distribution='normal'`.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        NaNs are treated as missing values: disregarded in ``fit``, and maintained\n",
      "        in ``transform``.\n",
      "\n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "\n",
      "        .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n",
      "               improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n",
      "               (2000).\n",
      "\n",
      "        .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n",
      "               of the Royal Statistical Society B, 26, 211-252 (1964).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.preprocessing import power_transform\n",
      "        >>> data = [[1, 2], [3, 2], [4, 5]]\n",
      "        >>> print(power_transform(data, method='box-cox'))\n",
      "        [[-1.332 -0.707]\n",
      "         [ 0.256 -0.707]\n",
      "         [ 1.076  1.414]]\n",
      "\n",
      "        .. warning:: Risk of data leak.\n",
      "            Do not use :func:`~sklearn.preprocessing.power_transform` unless you\n",
      "            know what you are doing. A common mistake is to apply it to the entire\n",
      "            data *before* splitting into training and test sets. This will bias the\n",
      "            model evaluation because information would have leaked from the test\n",
      "            set to the training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.PowerTransformer` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking, e.g.: `pipe = make_pipeline(PowerTransformer(),\n",
      "            LogisticRegression())`.\n",
      "\n",
      "    quantile_transform(X, *, axis=0, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=True)\n",
      "        Transform features using quantiles information.\n",
      "\n",
      "        This method transforms the features to follow a uniform or a normal\n",
      "        distribution. Therefore, for a given feature, this transformation tends\n",
      "        to spread out the most frequent values. It also reduces the impact of\n",
      "        (marginal) outliers: this is therefore a robust preprocessing scheme.\n",
      "\n",
      "        The transformation is applied on each feature independently. First an\n",
      "        estimate of the cumulative distribution function of a feature is\n",
      "        used to map the original values to a uniform distribution. The obtained\n",
      "        values are then mapped to the desired output distribution using the\n",
      "        associated quantile function. Features values of new/unseen data that fall\n",
      "        below or above the fitted range will be mapped to the bounds of the output\n",
      "        distribution. Note that this transform is non-linear. It may distort linear\n",
      "        correlations between variables measured at the same scale but renders\n",
      "        variables measured at different scales more directly comparable.\n",
      "\n",
      "        Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform.\n",
      "\n",
      "        axis : int, default=0\n",
      "            Axis used to compute the means and standard deviations along. If 0,\n",
      "            transform each feature, otherwise (if 1) transform each sample.\n",
      "\n",
      "        n_quantiles : int, default=1000 or n_samples\n",
      "            Number of quantiles to be computed. It corresponds to the number\n",
      "            of landmarks used to discretize the cumulative distribution function.\n",
      "            If n_quantiles is larger than the number of samples, n_quantiles is set\n",
      "            to the number of samples as a larger number of quantiles does not give\n",
      "            a better approximation of the cumulative distribution function\n",
      "            estimator.\n",
      "\n",
      "        output_distribution : {'uniform', 'normal'}, default='uniform'\n",
      "            Marginal distribution for the transformed data. The choices are\n",
      "            'uniform' (default) or 'normal'.\n",
      "\n",
      "        ignore_implicit_zeros : bool, default=False\n",
      "            Only applies to sparse matrices. If True, the sparse entries of the\n",
      "            matrix are discarded to compute the quantile statistics. If False,\n",
      "            these entries are treated as zeros.\n",
      "\n",
      "        subsample : int or None, default=1e5\n",
      "            Maximum number of samples used to estimate the quantiles for\n",
      "            computational efficiency. Note that the subsampling procedure may\n",
      "            differ for value-identical sparse and dense matrices.\n",
      "            Disable subsampling by setting `subsample=None`.\n",
      "\n",
      "            .. versionadded:: 1.5\n",
      "               The option `None` to disable subsampling was added.\n",
      "\n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for subsampling and smoothing\n",
      "            noise.\n",
      "            Please see ``subsample`` for more details.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "\n",
      "        copy : bool, default=True\n",
      "            If False, try to avoid a copy and transform in place.\n",
      "            This is not guaranteed to always work in place; e.g. if the data is\n",
      "            a numpy array with an int dtype, a copy will be returned even with\n",
      "            copy=False.\n",
      "\n",
      "            .. versionchanged:: 0.23\n",
      "                The default value of `copy` changed from False to True in 0.23.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        QuantileTransformer : Performs quantile-based scaling using the\n",
      "            Transformer API (e.g. as part of a preprocessing\n",
      "            :class:`~sklearn.pipeline.Pipeline`).\n",
      "        power_transform : Maps data to a normal distribution using a\n",
      "            power transformation.\n",
      "        scale : Performs standardization that is faster, but less robust\n",
      "            to outliers.\n",
      "        robust_scale : Performs robust standardization that removes the influence\n",
      "            of outliers but does not put outliers and inliers on the same scale.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "        transform.\n",
      "\n",
      "        .. warning:: Risk of data leak\n",
      "\n",
      "            Do not use :func:`~sklearn.preprocessing.quantile_transform` unless\n",
      "            you know what you are doing. A common mistake is to apply it\n",
      "            to the entire data *before* splitting into training and\n",
      "            test sets. This will bias the model evaluation because\n",
      "            information would have leaked from the test set to the\n",
      "            training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.QuantileTransformer` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking:`pipe = make_pipeline(QuantileTransformer(),\n",
      "            LogisticRegression())`.\n",
      "\n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.preprocessing import quantile_transform\n",
      "        >>> rng = np.random.RandomState(0)\n",
      "        >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n",
      "        >>> quantile_transform(X, n_quantiles=10, random_state=0, copy=True)\n",
      "        array([...])\n",
      "\n",
      "    robust_scale(X, *, axis=0, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)\n",
      "        Standardize a dataset along any axis.\n",
      "\n",
      "        Center to the median and component wise scale\n",
      "        according to the interquartile range.\n",
      "\n",
      "        Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_sample, n_features)\n",
      "            The data to center and scale.\n",
      "\n",
      "        axis : int, default=0\n",
      "            Axis used to compute the medians and IQR along. If 0,\n",
      "            independently scale each feature, otherwise (if 1) scale\n",
      "            each sample.\n",
      "\n",
      "        with_centering : bool, default=True\n",
      "            If `True`, center the data before scaling.\n",
      "\n",
      "        with_scaling : bool, default=True\n",
      "            If `True`, scale the data to unit variance (or equivalently,\n",
      "            unit standard deviation).\n",
      "\n",
      "        quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,        default=(25.0, 75.0)\n",
      "            Quantile range used to calculate `scale_`. By default this is equal to\n",
      "            the IQR, i.e., `q_min` is the first quantile and `q_max` is the third\n",
      "            quantile.\n",
      "\n",
      "            .. versionadded:: 0.18\n",
      "\n",
      "        copy : bool, default=True\n",
      "            If False, try to avoid a copy and scale in place.\n",
      "            This is not guaranteed to always work in place; e.g. if the data is\n",
      "            a numpy array with an int dtype, a copy will be returned even with\n",
      "            copy=False.\n",
      "\n",
      "        unit_variance : bool, default=False\n",
      "            If `True`, scale data so that normally distributed features have a\n",
      "            variance of 1. In general, if the difference between the x-values of\n",
      "            `q_max` and `q_min` for a standard normal distribution is greater\n",
      "            than 1, the dataset will be scaled down. If less than 1, the dataset\n",
      "            will be scaled up.\n",
      "\n",
      "            .. versionadded:: 0.24\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        RobustScaler : Performs centering and scaling using the Transformer API\n",
      "            (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This implementation will refuse to center scipy.sparse matrices\n",
      "        since it would make them non-sparse and would potentially crash the\n",
      "        program with memory exhaustion problems.\n",
      "\n",
      "        Instead the caller is expected to either set explicitly\n",
      "        `with_centering=False` (in that case, only variance scaling will be\n",
      "        performed on the features of the CSR matrix) or to call `X.toarray()`\n",
      "        if he/she expects the materialized dense array to fit in memory.\n",
      "\n",
      "        To avoid memory copy the caller should pass a CSR matrix.\n",
      "\n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n",
      "\n",
      "        .. warning:: Risk of data leak\n",
      "\n",
      "            Do not use :func:`~sklearn.preprocessing.robust_scale` unless you know\n",
      "            what you are doing. A common mistake is to apply it to the entire data\n",
      "            *before* splitting into training and test sets. This will bias the\n",
      "            model evaluation because information would have leaked from the test\n",
      "            set to the training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.RobustScaler` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking: `pipe = make_pipeline(RobustScaler(), LogisticRegression())`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.preprocessing import robust_scale\n",
      "        >>> X = [[-2, 1, 2], [-1, 0, 1]]\n",
      "        >>> robust_scale(X, axis=0)  # scale each column independently\n",
      "        array([[-1.,  1.,  1.],\n",
      "               [ 1., -1., -1.]])\n",
      "        >>> robust_scale(X, axis=1)  # scale each row independently\n",
      "        array([[-1.5,  0. ,  0.5],\n",
      "               [-1. ,  0. ,  1. ]])\n",
      "\n",
      "    scale(X, *, axis=0, with_mean=True, with_std=True, copy=True)\n",
      "        Standardize a dataset along any axis.\n",
      "\n",
      "        Center to the mean and component wise scale to unit variance.\n",
      "\n",
      "        Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to center and scale.\n",
      "\n",
      "        axis : {0, 1}, default=0\n",
      "            Axis used to compute the means and standard deviations along. If 0,\n",
      "            independently standardize each feature, otherwise (if 1) standardize\n",
      "            each sample.\n",
      "\n",
      "        with_mean : bool, default=True\n",
      "            If True, center the data before scaling.\n",
      "\n",
      "        with_std : bool, default=True\n",
      "            If True, scale the data to unit variance (or equivalently,\n",
      "            unit standard deviation).\n",
      "\n",
      "        copy : bool, default=True\n",
      "            If False, try to avoid a copy and scale in place.\n",
      "            This is not guaranteed to always work in place; e.g. if the data is\n",
      "            a numpy array with an int dtype, a copy will be returned even with\n",
      "            copy=False.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        StandardScaler : Performs scaling to unit variance using the Transformer\n",
      "            API (e.g. as part of a preprocessing\n",
      "            :class:`~sklearn.pipeline.Pipeline`).\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This implementation will refuse to center scipy.sparse matrices\n",
      "        since it would make them non-sparse and would potentially crash the\n",
      "        program with memory exhaustion problems.\n",
      "\n",
      "        Instead the caller is expected to either set explicitly\n",
      "        `with_mean=False` (in that case, only variance scaling will be\n",
      "        performed on the features of the CSC matrix) or to call `X.toarray()`\n",
      "        if he/she expects the materialized dense array to fit in memory.\n",
      "\n",
      "        To avoid memory copy the caller should pass a CSC matrix.\n",
      "\n",
      "        NaNs are treated as missing values: disregarded to compute the statistics,\n",
      "        and maintained during the data transformation.\n",
      "\n",
      "        We use a biased estimator for the standard deviation, equivalent to\n",
      "        `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      "        affect model performance.\n",
      "\n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n",
      "\n",
      "        .. warning:: Risk of data leak\n",
      "\n",
      "            Do not use :func:`~sklearn.preprocessing.scale` unless you know\n",
      "            what you are doing. A common mistake is to apply it to the entire data\n",
      "            *before* splitting into training and test sets. This will bias the\n",
      "            model evaluation because information would have leaked from the test\n",
      "            set to the training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.StandardScaler` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking: `pipe = make_pipeline(StandardScaler(), LogisticRegression())`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.preprocessing import scale\n",
      "        >>> X = [[-2, 1, 2], [-1, 0, 1]]\n",
      "        >>> scale(X, axis=0)  # scaling each column independently\n",
      "        array([[-1.,  1.,  1.],\n",
      "               [ 1., -1., -1.]])\n",
      "        >>> scale(X, axis=1)  # scaling each row independently\n",
      "        array([[-1.37,  0.39,  0.98],\n",
      "               [-1.22,  0.     ,  1.22]])\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Binarizer', 'KernelCenterer', 'MaxAbsScaler', 'MinMaxScale...\n",
      "\n",
      "FILE\n",
      "    /home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.preprocessing._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b3608cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RobustScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class RobustScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  RobustScaler(*, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)\n",
      " |\n",
      " |  Scale features using statistics that are robust to outliers.\n",
      " |\n",
      " |  This Scaler removes the median and scales the data according to\n",
      " |  the quantile range (defaults to IQR: Interquartile Range).\n",
      " |  The IQR is the range between the 1st quartile (25th quantile)\n",
      " |  and the 3rd quartile (75th quantile).\n",
      " |\n",
      " |  Centering and scaling happen independently on each feature by\n",
      " |  computing the relevant statistics on the samples in the training\n",
      " |  set. Median and interquartile range are then stored to be used on\n",
      " |  later data using the :meth:`transform` method.\n",
      " |\n",
      " |  Standardization of a dataset is a common preprocessing for many machine\n",
      " |  learning estimators. Typically this is done by removing the mean and\n",
      " |  scaling to unit variance. However, outliers can often influence the sample\n",
      " |  mean / variance in a negative way. In such cases, using the median and the\n",
      " |  interquartile range often give better results. For an example visualization\n",
      " |  and comparison to other scalers, refer to :ref:`Compare RobustScaler with\n",
      " |  other scalers <plot_all_scaling_robust_scaler_section>`.\n",
      " |\n",
      " |  .. versionadded:: 0.17\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  with_centering : bool, default=True\n",
      " |      If `True`, center the data before scaling.\n",
      " |      This will cause :meth:`transform` to raise an exception when attempted\n",
      " |      on sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |\n",
      " |  with_scaling : bool, default=True\n",
      " |      If `True`, scale the data to interquartile range.\n",
      " |\n",
      " |  quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,         default=(25.0, 75.0)\n",
      " |      Quantile range used to calculate `scale_`. By default this is equal to\n",
      " |      the IQR, i.e., `q_min` is the first quantile and `q_max` is the third\n",
      " |      quantile.\n",
      " |\n",
      " |      .. versionadded:: 0.18\n",
      " |\n",
      " |  copy : bool, default=True\n",
      " |      If `False`, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |\n",
      " |  unit_variance : bool, default=False\n",
      " |      If `True`, scale data so that normally distributed features have a\n",
      " |      variance of 1. In general, if the difference between the x-values of\n",
      " |      `q_max` and `q_min` for a standard normal distribution is greater\n",
      " |      than 1, the dataset will be scaled down. If less than 1, the dataset\n",
      " |      will be scaled up.\n",
      " |\n",
      " |      .. versionadded:: 0.24\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  center_ : array of floats\n",
      " |      The median value for each feature in the training set.\n",
      " |\n",
      " |  scale_ : array of floats\n",
      " |      The (scaled) interquartile range for each feature in the training set.\n",
      " |\n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |\n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |\n",
      " |      .. versionadded:: 0.24\n",
      " |\n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |\n",
      " |      .. versionadded:: 1.0\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  robust_scale : Equivalent function without the estimator API.\n",
      " |  sklearn.decomposition.PCA : Further removes the linear correlation across\n",
      " |      features with 'whiten=True'.\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |\n",
      " |  https://en.wikipedia.org/wiki/Median\n",
      " |  https://en.wikipedia.org/wiki/Interquartile_range\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import RobustScaler\n",
      " |  >>> X = [[ 1., -2.,  2.],\n",
      " |  ...      [ -2.,  1.,  3.],\n",
      " |  ...      [ 4.,  1., -2.]]\n",
      " |  >>> transformer = RobustScaler().fit(X)\n",
      " |  >>> transformer\n",
      " |  RobustScaler()\n",
      " |  >>> transformer.transform(X)\n",
      " |  array([[ 0. , -2. ,  0. ],\n",
      " |         [-1. ,  0. ,  0.4],\n",
      " |         [ 1. ,  0. , -1.6]])\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      RobustScaler\n",
      " |      sklearn.base.OneToOneFeatureMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      " |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, *, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __sklearn_tags__(self)\n",
      " |\n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the median and quantiles to be used for scaling.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the median and quantiles\n",
      " |          used for later scaling along the features axis.\n",
      " |\n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |\n",
      " |  inverse_transform(self, X)\n",
      " |      Scale back the data to the original representation.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The rescaled data to be transformed back.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_original : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |\n",
      " |  transform(self, X)\n",
      " |      Center and scale the data.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to scale along the specified axis.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      " |\n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |\n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Same as input features.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |\n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |\n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |\n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |\n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set output container.\n",
      " |\n",
      " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      " |      for an example on how to use the API.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |\n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `\"polars\"`: Polars output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |\n",
      " |          .. versionadded:: 1.4\n",
      " |              `\"polars\"` option was added.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |\n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.preprocessing._data.RobustScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d1d3b",
   "metadata": {},
   "source": [
    "## Applying Scaling on DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5524c702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>symbol</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>vol_24h</th>\n",
       "      <th>total_vol</th>\n",
       "      <th>chg_24h</th>\n",
       "      <th>chg_7d</th>\n",
       "      <th>market_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-23 01:05:12</td>\n",
       "      <td>BNB</td>\n",
       "      <td>BNB</td>\n",
       "      <td>990.2</td>\n",
       "      <td>$4.87B</td>\n",
       "      <td>2.12%</td>\n",
       "      <td>-5.70%</td>\n",
       "      <td>+7.38%</td>\n",
       "      <td>$137.66B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-23 01:05:12</td>\n",
       "      <td>Dogecoin</td>\n",
       "      <td>DOGE</td>\n",
       "      <td>0.241003</td>\n",
       "      <td>$5.60B</td>\n",
       "      <td>2.48%</td>\n",
       "      <td>-8.69%</td>\n",
       "      <td>-10.26%</td>\n",
       "      <td>$36.21B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-23 01:05:12</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>4,191.67</td>\n",
       "      <td>$58.33B</td>\n",
       "      <td>24.46%</td>\n",
       "      <td>-6.29%</td>\n",
       "      <td>-7.10%</td>\n",
       "      <td>$506.47B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-23 01:05:12</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>BTC</td>\n",
       "      <td>112,796.60</td>\n",
       "      <td>$70.20B</td>\n",
       "      <td>28.66%</td>\n",
       "      <td>-2.13%</td>\n",
       "      <td>-2.16%</td>\n",
       "      <td>$2.25T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-23 01:05:12</td>\n",
       "      <td>XRP</td>\n",
       "      <td>XRP</td>\n",
       "      <td>2.8519</td>\n",
       "      <td>$10.05B</td>\n",
       "      <td>4.22%</td>\n",
       "      <td>-4.49%</td>\n",
       "      <td>-5.22%</td>\n",
       "      <td>$170.09B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94441</th>\n",
       "      <td>2025-03-17 00:34:14</td>\n",
       "      <td>Solana</td>\n",
       "      <td>SOL</td>\n",
       "      <td>127.675</td>\n",
       "      <td>$2.36B</td>\n",
       "      <td>3.90%</td>\n",
       "      <td>-6.14%</td>\n",
       "      <td>-0.46%</td>\n",
       "      <td>$64.91B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94442</th>\n",
       "      <td>2025-03-17 00:34:14</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>BTC</td>\n",
       "      <td>82,953.80</td>\n",
       "      <td>$19.36B</td>\n",
       "      <td>31.82%</td>\n",
       "      <td>-1.66%</td>\n",
       "      <td>+0.45%</td>\n",
       "      <td>$1.64T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94443</th>\n",
       "      <td>2025-03-17 00:34:14</td>\n",
       "      <td>XRP</td>\n",
       "      <td>XRP</td>\n",
       "      <td>2.3201</td>\n",
       "      <td>$3.85B</td>\n",
       "      <td>6.82%</td>\n",
       "      <td>-4.14%</td>\n",
       "      <td>+6.16%</td>\n",
       "      <td>$134.52B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94444</th>\n",
       "      <td>2025-03-17 00:34:14</td>\n",
       "      <td>Tether USDt</td>\n",
       "      <td>USDT</td>\n",
       "      <td>1</td>\n",
       "      <td>$46.75B</td>\n",
       "      <td>79.72%</td>\n",
       "      <td>-0.01%</td>\n",
       "      <td>+0.00%</td>\n",
       "      <td>$143.45B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94445</th>\n",
       "      <td>2025-03-17 00:34:14</td>\n",
       "      <td>USDC</td>\n",
       "      <td>USDC</td>\n",
       "      <td>1</td>\n",
       "      <td>$5.16B</td>\n",
       "      <td>8.43%</td>\n",
       "      <td>+0.01%</td>\n",
       "      <td>+0.00%</td>\n",
       "      <td>$58.72B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94446 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 timestamp         name symbol   price_usd  vol_24h total_vol  \\\n",
       "0      2025-09-23 01:05:12          BNB    BNB       990.2   $4.87B     2.12%   \n",
       "1      2025-09-23 01:05:12     Dogecoin   DOGE    0.241003   $5.60B     2.48%   \n",
       "2      2025-09-23 01:05:12     Ethereum    ETH    4,191.67  $58.33B    24.46%   \n",
       "3      2025-09-23 01:05:12      Bitcoin    BTC  112,796.60  $70.20B    28.66%   \n",
       "4      2025-09-23 01:05:12          XRP    XRP      2.8519  $10.05B     4.22%   \n",
       "...                    ...          ...    ...         ...      ...       ...   \n",
       "94441  2025-03-17 00:34:14       Solana    SOL     127.675   $2.36B     3.90%   \n",
       "94442  2025-03-17 00:34:14      Bitcoin    BTC   82,953.80  $19.36B    31.82%   \n",
       "94443  2025-03-17 00:34:14          XRP    XRP      2.3201   $3.85B     6.82%   \n",
       "94444  2025-03-17 00:34:14  Tether USDt   USDT           1  $46.75B    79.72%   \n",
       "94445  2025-03-17 00:34:14         USDC   USDC           1   $5.16B     8.43%   \n",
       "\n",
       "      chg_24h   chg_7d market_cap  \n",
       "0      -5.70%   +7.38%   $137.66B  \n",
       "1      -8.69%  -10.26%    $36.21B  \n",
       "2      -6.29%   -7.10%   $506.47B  \n",
       "3      -2.13%   -2.16%     $2.25T  \n",
       "4      -4.49%   -5.22%   $170.09B  \n",
       "...       ...      ...        ...  \n",
       "94441  -6.14%   -0.46%    $64.91B  \n",
       "94442  -1.66%   +0.45%     $1.64T  \n",
       "94443  -4.14%   +6.16%   $134.52B  \n",
       "94444  -0.01%   +0.00%   $143.45B  \n",
       "94445  +0.01%   +0.00%    $58.72B  \n",
       "\n",
       "[94446 rows x 9 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/workspaces/MonoRepo/DPEL/FILES/cryptocurrency.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdcdd16",
   "metadata": {},
   "source": [
    "#### Preprocessing the price_usd to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7726dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comma(val: pd.Series) -> float:\n",
    "    if '$' in val:\n",
    "        val = val.replace('$','')\n",
    "    if \"B\" in val:\n",
    "        val = val.replace('B','')\n",
    "        val = float(val) * 10**9\n",
    "        return val\n",
    "    if \"M\" in val:\n",
    "        val = val.replace('M','')\n",
    "        val = float(val) * 10**6\n",
    "        return val\n",
    "    if ',' in val:\n",
    "        val = val.replace(',','')\n",
    "        val = float(val)\n",
    "        return val\n",
    "    if '%' in val:\n",
    "        val = np.nan\n",
    "        return val\n",
    "    else:\n",
    "        val = float(val)\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a07a1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"updated_usd\"] = df[\"price_usd\"].apply(remove_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66f7d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.updated_usd.dropna().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c1b5e",
   "metadata": {},
   "source": [
    "### MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98bb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00824762e-08],\n",
       "       [2.45385317e-12],\n",
       "       [4.26806841e-08],\n",
       "       ...,\n",
       "       [2.36237646e-11],\n",
       "       [1.01821599e-11],\n",
       "       [1.01821599e-11]], shape=(94430, 1))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar = MinMaxScaler()\n",
    "minmaxScaled = scalar.fit_transform(new_df)\n",
    "minmaxScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41b730c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00587655],\n",
       "       [-0.00587915],\n",
       "       [-0.00586812],\n",
       "       ...,\n",
       "       [-0.00587915],\n",
       "       [-0.00587915],\n",
       "       [-0.00587915]], shape=(94430, 1))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar = StandardScaler()\n",
    "standard_data = scalar.fit_transform(new_df)\n",
    "standard_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3385b8",
   "metadata": {},
   "source": [
    "## Robust Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cf8f6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.56873833e+00],\n",
       "       [-2.74707755e-02],\n",
       "       [ 3.63683802e+01],\n",
       "       ...,\n",
       "       [-9.41714637e-03],\n",
       "       [-2.08801016e-02],\n",
       "       [-2.08801016e-02]], shape=(94430, 1))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar = RobustScaler()\n",
    "robust_data = scalar.fit_transform(new_df)\n",
    "robust_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
